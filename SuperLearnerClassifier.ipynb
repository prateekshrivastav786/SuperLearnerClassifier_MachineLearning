{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import display, HTML, Image\n",
    "import io\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the Super learner class which train the base classifier and stack layer classifier. All the method associated with the stack\n",
    "layer classifier is encpsulated in the class. It will call normally as anyother classifier. After calling this classifier, the\n",
    "same object can be used in calling fit, predict and predict_proba method.\n",
    "\"\"\"\n",
    "\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, k=5, stackclf= tree.DecisionTreeClassifier(), pre_pro= False, Ori_Des = False):\n",
    "    \n",
    "        self.stackclf = stackclf   # This is the class constucter. Taking values from the call and handel here\n",
    "        self.pre_pro = pre_pro\n",
    "        self.k = k\n",
    "        self.Ori_Des= Ori_Des\n",
    "     \n",
    "    def fit(self, X,y):  \n",
    "        \n",
    "# This  is a Fit method used to fit all the classifier. It fits the stack layer classifier on output of the base classifiers\n",
    "        \n",
    "        newpdt = pd.DataFrame()      # Taking data frame \n",
    "        newplr = pd.DataFrame()\n",
    "        newpknn = pd.DataFrame()\n",
    "        newprf = pd.DataFrame()\n",
    "        newpnb = pd.DataFrame()\n",
    "        newpbag = pd.DataFrame()\n",
    "        bag =[]\n",
    "        nb = []\n",
    "        rrf = []\n",
    "        rdt = []\n",
    "        rLR = []\n",
    "        rKNN = []\n",
    "        self.label = y\n",
    "       \n",
    "        kf = KFold(n_splits=self.k)   # this is K vold validation code which split the data into folds\n",
    "        kf.get_n_splits(X)\n",
    "        xv = X.values\n",
    "\n",
    "        KFold(n_splits=self.k, random_state=None, shuffle=False)\n",
    "       \n",
    "        for train_index, test_index in kf.split(X):\n",
    " \n",
    "\n",
    "            X_train, X_test = xv[train_index], xv[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "# Here fitting the classifiers using K fold\n",
    "            self.DT_trees = tree.DecisionTreeClassifier(criterion=\"gini\", min_samples_split = 200, max_depth=9)\n",
    "            self.DT_trees.fit(X_train,y_train)\n",
    "            \n",
    "            self.Log_Reg = linear_model.LogisticRegression()\n",
    "            self.Log_Reg = self.Log_Reg.fit(X_train,y_train)\n",
    "                         \n",
    "            self.KNNCla = neighbors.KNeighborsClassifier()\n",
    "            self.KNNCla = self.KNNCla.fit(X_train,y_train)\n",
    "            \n",
    "            self.Ran_For = ensemble.RandomForestClassifier(n_estimators=300, max_features = 3, min_samples_split=200)\n",
    "            self.Ran_For.fit(X_train,y_train)\n",
    "            \n",
    "            self.Nbc = GaussianNB()                # weak classifier\n",
    "            self.Nbc = self.Nbc.fit(X_train,y_train)\n",
    "                       \n",
    "            self.bagclf = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                      n_estimators=10)\n",
    "            self.bagclf.fit(X_train,y_train)\n",
    "# getting the predictions below on the trained classifiers            \n",
    "            label_d14 = self.DT_trees.predict(X_test)\n",
    "            rdt.append(label_d14)\n",
    "            label_predlrr = self.Log_Reg.predict(X_test)\n",
    "            rLR.append(label_predlrr)\n",
    "            label_KNNn = self.KNNCla.predict(X_test)\n",
    "            rKNN.append(label_KNNn)                   \n",
    "            Ran_pre = self.Ran_For.predict(X_test)\n",
    "            rrf.append(Ran_pre)\n",
    "            Nbc_pre = self.Nbc.predict(X_test)\n",
    "            nb.append(Nbc_pre)\n",
    "            bag_pred = self.bagclf.predict(X_test)\n",
    "            bag.append(bag_pred)\n",
    "\n",
    "# Getting the probability output on the fitted classifiers\n",
    "\n",
    "            pp1 = self.DT_trees.predict_proba(X_test)\n",
    "            pdtt = pd.DataFrame(pp1)\n",
    "            v1 = [\"M_\"+ str(i) for i in range(1,11)]  # these v value for column names\n",
    "            pdtt.columns =v1\n",
    "                   \n",
    "            pp2 = self.Log_Reg.predict_proba(X_test)\n",
    "            pplr= pd.DataFrame(pp2)\n",
    "            v2 = [\"M_\"+ str(i) for i in range(11,21)]\n",
    "            pplr.columns =v2\n",
    "            \n",
    "            pp3 = self.KNNCla.predict_proba(X_test)\n",
    "            ppknn= pd.DataFrame(pp3)\n",
    "            v3 = [\"M_\"+ str(i) for i in range(21,31)]\n",
    "            ppknn.columns =v3\n",
    "               \n",
    "            pp4 = self.Ran_For.predict_proba(X_test)\n",
    "            pprf= pd.DataFrame(pp4)\n",
    "            v4 = [\"M_\"+ str(i) for i in range(31,41)]\n",
    "            pprf.columns =v4\n",
    "            \n",
    "            pp5 = self.Nbc.predict_proba(X_test)\n",
    "            ppnb= pd.DataFrame(pp5)\n",
    "            v5 = [\"M_\"+ str(i) for i in range(41,51)]\n",
    "            ppnb.columns =v5\n",
    "            \n",
    "            pp6 = self.bagclf.predict_proba(X_test)\n",
    "            ppbag= pd.DataFrame(pp6)\n",
    "            v6 = [\"M_\"+ str(i) for i in range(51,61)]\n",
    "            ppbag.columns =v6\n",
    "            \n",
    "            newpdt= pd.concat([newpdt, pdtt])\n",
    "            newplr= pd.concat([newplr, pplr])\n",
    "            newpknn= pd.concat([newpknn, ppknn])\n",
    "            newprf= pd.concat([newprf, pprf])\n",
    "            newpnb= pd.concat([newpnb, ppnb])\n",
    "            newpbag= pd.concat([newpbag, ppbag])\n",
    "            \n",
    "            \n",
    "# here I am fitting the classifier with the whole X and y as last fold of K fold I applied above have trained on 75% of the data only so filling the gap I am again training the clf on fully data too.            \n",
    "        self.DT_trees= self.DT_trees.fit(X,y)\n",
    "        self.Log_Reg = self.Log_Reg.fit(X,y)\n",
    "        self.KNNCla = self.KNNCla.fit(X,y)\n",
    "        self.Ran_For = self.Ran_For.fit(X,y)\n",
    "        self.Nbc = self.Nbc.fit(X,y)\n",
    "        self.bagclf = self.bagclf.fit(X,y)\n",
    "        \n",
    "# Here concating the predicyed values from predict_proba method of the base classifiers                   \n",
    "            \n",
    "        final_probip = pd.concat([newpdt, newplr, newpknn, newprf, newpnb, newpbag], axis =1)\n",
    "        final_probip[\"label\"] = self.label\n",
    "       \n",
    "        FXp = final_probip.iloc[:,:-1]\n",
    "        Fyp = final_probip[\"label\"]\n",
    "        X=X.reset_index(drop=True)\n",
    "        final_probip=final_probip.reset_index(drop=True)\n",
    "        \n",
    "        With_Orig = pd.concat([X,final_probip], axis =1)\n",
    "        With_Orig[\"label\"] = self.label\n",
    "        \n",
    "        FXo = With_Orig.iloc[:,:-1]\n",
    "        Fyo = With_Orig[\"label\"]\n",
    "              \n",
    " # for better use of the predicted value I have put them into the pandas data frame and used the array to append the predicted value of each iteration of k fold and then for taking them row bise I have used the reshape feature of numpy array            \n",
    "        \n",
    "        DtpreData = pd.DataFrame((np.array(rdt)).reshape(np.array(rdt).size, 1))\n",
    "        LrpreData = pd.DataFrame((np.array(rLR)).reshape(np.array(rLR).size, 1))\n",
    "        KnnpreData = pd.DataFrame((np.array(rKNN)).reshape(np.array(rKNN).size, 1))\n",
    "        RafpreData = pd.DataFrame((np.array(rrf)).reshape(np.array(rrf).size, 1))\n",
    "        NabpreData = pd.DataFrame((np.array(nb)).reshape(np.array(nb).size, 1))    \n",
    "        BagpreData = pd.DataFrame((np.array(bag)).reshape(np.array(bag).size, 1))\n",
    "\n",
    "# Merging the output of the predict method         \n",
    "        PreInp = pd.concat([self.label, DtpreData, LrpreData, KnnpreData, RafpreData, NabpreData, BagpreData], axis =1)\n",
    "        PreInp.columns = [\"Ori_Labels\",\"DecTree\", \"LR\", \"KNN\", \"RandFor\", \"NaiveBa\", \"Bagging\"]\n",
    "        FX = PreInp.iloc[:,1:]      # extracting the X and y from the data\n",
    "        Fy = PreInp[\"Ori_Labels\"]\n",
    "        PreInp=PreInp.reset_index(drop=True)\n",
    "        \n",
    "        With_Orig1 = pd.concat([PreInp, X], axis =1)\n",
    "        FXpreo = With_Orig1.iloc[:,1:]    # Extracting the X and y from the data which has Orignal feature\n",
    "        Fypreo = With_Orig1.iloc[:,:1]\n",
    "        \n",
    "# here I have copied the classifier instance for predict and predict_proba method using the deepcopy feature       \n",
    "        self.dtpre = copy.deepcopy(self.DT_trees)\n",
    "        self.lrpre = copy.deepcopy(self.Log_Reg)\n",
    "        self.knnpre = copy.deepcopy(self.KNNCla)\n",
    "        self.Rfpre = copy.deepcopy(self.Ran_For)\n",
    "        self.Nbpre = copy.deepcopy(self.Nbc)\n",
    "        self.bagipre = copy.deepcopy(self.bagclf)\n",
    "                               \n",
    "        self.dtpre1 = copy.deepcopy(self.DT_trees)\n",
    "        self.lrpre1 = copy.deepcopy(self.Log_Reg)\n",
    "        self.knnpre1 = copy.deepcopy(self.KNNCla)\n",
    "        self.Rfpre1 = copy.deepcopy(self.Ran_For)\n",
    "        self.Nbpre1 = copy.deepcopy(self.Nbc)\n",
    "        self.bagipre1 = copy.deepcopy(self.bagclf)\n",
    "        \n",
    "        self.PPdtpre = copy.deepcopy(self.DT_trees)\n",
    "        self.PPlrpre = copy.deepcopy(self.Log_Reg)\n",
    "        self.PPknnpre = copy.deepcopy(self.KNNCla)\n",
    "        self.PPRfpre = copy.deepcopy(self.Ran_For)\n",
    "        self.PPNbpre = copy.deepcopy(self.Nbc)\n",
    "        self.PPbagipre = copy.deepcopy(self.bagclf)\n",
    "        \n",
    "\n",
    "        \n",
    "# Here I have made the copy for the stack classifier which gonna use in below conditions.\n",
    "        stackp = copy.deepcopy(self.stackclf)\n",
    "        stack1 = copy.deepcopy(self.stackclf)\n",
    "        stackpro = copy.deepcopy(self.stackclf)       \n",
    "        stackpro1 = copy.deepcopy(self.stackclf)\n",
    "        \n",
    "# Here is the main codition whether the stack classifier is Decision tree or Logistic Regression\n",
    "# Whether to use the output of Predict method or predict_proba method to fit the stack classifier\n",
    "# Whether to take the Orignal descriptive feature or not\n",
    "        \n",
    "        if self.pre_pro == True:         # this is condition for predict or predict_proba method\n",
    "            if self.Ori_Des == True:             # To take Orignal Descriptive Feature or not\n",
    "                self.DT_treesStaProOrg= stackpro.fit(FXo,Fyo)       \n",
    "            else:\n",
    "                self.DT_treesStaProNorg= stackpro1.fit(FXp,Fyp)   #stack classifier # predict proba\n",
    "                \n",
    "            \n",
    "        elif self.pre_pro == False:\n",
    "            if self.Ori_Des == True:\n",
    "                self.DT_treesStaPreOrg= stackp.fit(FXpreo,Fypreo)\n",
    "            else:\n",
    "                self.DT_treesStaPreNorg= stack1.fit(FX,Fy)   # stack classifier predict waala\n",
    "                              \n",
    "        else:\n",
    "                                  \n",
    "            self.DT_treesStaPre= stack1.fit(FX,Fy)   # This is for the default use of stack classifier i.e. Decision Tree trained on predicted output with no Orignal descriptive feature \n",
    "\n",
    "        \n",
    "        return self\n",
    "   \n",
    "    def predict(self, xtest):\n",
    "        \n",
    "# This is a predict method which predict the values on classifiers. I have take the predict and predict_proba method here.\n",
    "# To predict the value according to the condition I have taken predict and predict_proba method's prediction output\n",
    "# Also,I have added the orignal feature of the input data to this method for task 8\n",
    "                \n",
    "        label_d14 = self.dtpre.predict(xtest)\n",
    "        dtp= pd.DataFrame(label_d14)\n",
    "        \n",
    "        label_predlrr = self.lrpre.predict(xtest)\n",
    "        lrp = pd.DataFrame(label_predlrr)\n",
    "        \n",
    "        label_KNNn = self.knnpre.predict(xtest)\n",
    "        knnp = pd.DataFrame(label_KNNn)\n",
    "        \n",
    "        Ran_pre = self.Ran_For.predict(xtest)\n",
    "        ranfp = pd.DataFrame(Ran_pre)\n",
    "        \n",
    "        Nbc_pre = self.Nbc.predict(xtest)\n",
    "        nbcp = pd.DataFrame(Nbc_pre)\n",
    "        \n",
    "        bag_pred = self.bagclf.predict(xtest)\n",
    "        bagp = pd.DataFrame(bag_pred)\n",
    "                \n",
    "# here Concating the values from predict method run on base classifiers        \n",
    "        PreInpF = pd.concat([dtp, lrp, knnp, ranfp, nbcp,bagp], axis =1)\n",
    "        PreInpF.columns = [\"DecTree\", \"LR\", \"KNN\", \"RanFor\", \"NaiveB\", \"Bagging\"]       \n",
    "           \n",
    "# for the condition I am predicting on the predict_proba method below  \n",
    "        dtppi = self.PPdtpre.predict_proba(xtest)\n",
    "        dtpp = pd.DataFrame(dtppi)\n",
    "                \n",
    "        lrppi = self.PPlrpre.predict_proba(xtest)\n",
    "        lrpp = pd.DataFrame(lrppi)\n",
    "        \n",
    "        knnppi = self.PPknnpre.predict_proba(xtest)\n",
    "        knnpp = pd.DataFrame(knnppi)\n",
    "        \n",
    "        Ran_prei = self.PPRfpre.predict_proba(xtest)\n",
    "        ranfpp = pd.DataFrame(Ran_prei)\n",
    "        \n",
    "        Nbc_prei = self.PPNbpre.predict_proba(xtest)\n",
    "        nbcpp = pd.DataFrame(Nbc_prei)\n",
    "        \n",
    "        bag_predi = self.PPbagipre.predict_proba(xtest)\n",
    "        bagpp = pd.DataFrame(bag_predi)\n",
    "        \n",
    "# Now concating the output of the predict_proba method for predicting on the stack classifiers                         \n",
    "        ProPreInp = pd.concat([dtpp, lrpp, knnpp, ranfpp, nbcpp, bagpp], axis =1)\n",
    "        \n",
    "        ProPreInp = ProPreInp.reset_index(drop=True)\n",
    "        xtest = xtest.reset_index(drop=True)\n",
    "        \n",
    "        Pro_Orig1 = pd.concat([ProPreInp, xtest], axis =1)   # Concating the predict_proba o/p with orignal featue passed to this method\n",
    "        \n",
    "                \n",
    "        Pre_Orig1 = pd.concat([PreInpF, xtest], axis =1)  # Concating the predict method output with Orignal feature\n",
    "        \n",
    "        \n",
    "# here are the condition for Probability, prediction and Orignal descriptive feature\n",
    "               \n",
    "        if self.pre_pro == True:       # this is for whether probability is to be taken or not\n",
    "            if self.Ori_Des == True:    # Thsi is for whether the Orignal feature is taken or not\n",
    "                \n",
    "                Final_Pred= self.DT_treesStaProOrg.predict(Pro_Orig1)\n",
    "                \n",
    "            else:\n",
    "                Final_Pred= self.DT_treesStaProNorg.predict(ProPreInp)\n",
    "                \n",
    "            \n",
    "        elif self.pre_pro == False:\n",
    "            if self.Ori_Des == True:\n",
    "                \n",
    "                Final_Pred= self.DT_treesStaPreOrg.predict(Pre_Orig1)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                Final_Pred= self.DT_treesStaPreNorg.predict(PreInpF)\n",
    "              \n",
    "        \n",
    "        return Final_Pred\n",
    "    \n",
    "    def predict_proba(self, xtest):\n",
    "        \n",
    "# This is the predict_proba method used for predicting the probability of the target feature. It given 10 output with probability value for being one targer feature or another\n",
    "# here I am using the predict and predict_proba method of the base classifiers to predict on the stack layer classifier on given condition                \n",
    "        \n",
    "        label_d = self.dtpre1.predict(xtest)\n",
    "        dp= pd.DataFrame(label_d)\n",
    "        \n",
    "        label_prelrr = self.lrpre1.predict(xtest)\n",
    "        lp = pd.DataFrame(label_prelrr)\n",
    "        \n",
    "        label_KNn = self.knnpre1.predict(xtest)\n",
    "        knp = pd.DataFrame(label_KNn)\n",
    "        \n",
    "        Ran_pre = self.Ran_For.predict(xtest)\n",
    "        ranfp = pd.DataFrame(Ran_pre)\n",
    "        \n",
    "        Nbc_pre = self.Nbc.predict(xtest)\n",
    "        nbcp = pd.DataFrame(Nbc_pre)\n",
    "        \n",
    "        bag_pred = self.bagclf.predict(xtest)\n",
    "        bagp = pd.DataFrame(bag_pred)\n",
    "                \n",
    "# merging the output of the predict method \n",
    "        PreInpF = pd.concat([dp, lp, knp, ranfp, nbcp,bagp], axis =1)\n",
    "        PreInpF.columns = [\"DecTree\", \"LR\", \"KNN\", \"RanFor\", \"NaiveB\", \"Bagging\"]     \n",
    "                \n",
    "        dtppi = self.PPdtpre.predict_proba(xtest)  # Predicting the values on base classifier's predict_proba method\n",
    "        dtpp = pd.DataFrame(dtppi)\n",
    "                \n",
    "        lrppi = self.PPlrpre.predict_proba(xtest)\n",
    "        lrpp = pd.DataFrame(lrppi)\n",
    "        \n",
    "        knnppi = self.PPknnpre.predict_proba(xtest)\n",
    "        knnpp = pd.DataFrame(knnppi)\n",
    "        \n",
    "        Ran_prei = self.PPRfpre.predict_proba(xtest)\n",
    "        ranfpp = pd.DataFrame(Ran_prei)\n",
    "        \n",
    "        Nbc_prei = self.PPNbpre.predict_proba(xtest)\n",
    "        nbcpp = pd.DataFrame(Nbc_prei)\n",
    "        \n",
    "        bag_predi = self.PPbagipre.predict_proba(xtest)\n",
    "        bagpp = pd.DataFrame(bag_predi)\n",
    "        \n",
    "# concating the predict_proba output of the base classifiers\n",
    "        \n",
    "        ProPreInp = pd.concat([dtpp, lrpp, knnpp, ranfpp, nbcpp, bagpp], axis =1)\n",
    "        \n",
    "        ProPreInp = ProPreInp.reset_index(drop=True)\n",
    "        xtest = xtest.reset_index(drop=True)\n",
    "        \n",
    "        Pro_Orig1 = pd.concat([ProPreInp, xtest], axis =1)    # This is a merger of Orignal feature and Predict_proba method output of the base learner\n",
    "        #PreInpF = PreInpF.reset_index(drop=True)\n",
    "                \n",
    "        Pre_Orig1 = pd.concat([PreInpF, xtest], axis =1)        # this is merger of orignal feature and predict method output of the base learner\n",
    "        \n",
    "        if self.pre_pro == True:\n",
    "            if self.Ori_Des == True:\n",
    "                Final_Pred= self.DT_treesStaProOrg.predict_proba(Pro_Orig1)\n",
    "                \n",
    "            else:\n",
    "                Final_Pred= self.DT_treesStaProNorg.predict_proba(ProPreInp)\n",
    "                \n",
    "            \n",
    "        elif self.pre_pro == False:\n",
    "            if self.Ori_Des == True:\n",
    "                                \n",
    "                Final_Pred= self.DT_treesStaPreOrg.predict_proba(Pre_Orig1)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                Final_Pred= self.DT_treesStaPreNorg.predict_proba(PreInpF)\n",
    "                \n",
    "                        \n",
    "        \n",
    "        return Final_Pred\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the Super learner classifier are described below:\n",
    "K = it is a k fold value. by default it is set as 5 in the class above. Range 5-10 is good.\n",
    "stackclf = It is a stack layer classifier aka final classifier it takes either of two values from tree.DecisionTreeClassifier() and linear_model.LogisticRegression() and use for the stack layer. Make sure it is imported from the scikit learn packages first. By default it is set as tree.DecisionTreeClassifier()\n",
    "pre_pro = It is check for whether stack layer takes the input from the Predict Method or Predict_proba method of the base classifiers. By default is set as false i.e. Prediction method of base classifiers will be used to fit the stack model.\n",
    "Ori_Des = This is a check for whether to include the Orignal descrptive features or not. By default it is set as false i.e. Orignal features will not be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To run the below code, run the code written in Pre-process & Partition Data section to load the data first and partions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stack DT: 0.808333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateek\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stack LR: 0.941666666667\n"
     ]
    }
   ],
   "source": [
    "clfdt = SuperLearnerClassifier(k =2, stackclf= tree.DecisionTreeClassifier(), pre_pro= False, Ori_Des = False)\n",
    "clfdt.fit(X,y)\n",
    "clfdtpre=clfdt.predict(X_test1)\n",
    "clfdt.predict_proba(X_test1)\n",
    "accuracy1 = metrics.accuracy_score(y_test1, clfdtpre)\n",
    "print(\"Accuracy of Stack DT: \" +  str(accuracy1))\n",
    "\n",
    "clflr = SuperLearnerClassifier(k=2, stackclf=linear_model.LogisticRegression(), pre_pro=False, Ori_Des=True)\n",
    "clflr.fit(X,y)\n",
    "clfpre=clflr.predict(X_test1)\n",
    "clflr.predict_proba(X_test1)\n",
    "\n",
    "accuracy2 = metrics.accuracy_score(y_test1, clfpre)\n",
    "print(\"Accuracy of Stack LR: \" +  str(accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take sample from the dataset so everyhting runs smoothly define in above cell\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "display(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= dataset[\"label\"]\n",
    "X = dataset.iloc[:,1:]\n",
    "X = X/255     # for normalizing teh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition of the data is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y, random_state=0, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT stack layer: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateek\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LR Stack layer: 0.941666666667\n"
     ]
    }
   ],
   "source": [
    "#Calling the Super learner with the given parameters\n",
    "clf1 = SuperLearnerClassifier(stackclf= tree.DecisionTreeClassifier(), pre_pro= False, Ori_Des = True, k =2)\n",
    "clf1.fit(X,y)\n",
    "clf1pre=clf1.predict(X_test1)\n",
    "clf1.predict_proba(X_test1)\n",
    "accuracydtS = metrics.accuracy_score(y_test1, clf1pre) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy of DT stack layer: \" +  str(accuracydtS))\n",
    "\n",
    "clf2 = SuperLearnerClassifier(k=2, stackclf=linear_model.LogisticRegression(), pre_pro=False, Ori_Des=True)\n",
    "clf2.fit(X,y)\n",
    "clf2pre=clflr.predict(X_test1)\n",
    "clf2.predict_proba(X_test1)\n",
    "accuracylrS = metrics.accuracy_score(y_test1, clf2pre) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy of LR Stack layer: \" +  str(accuracylrS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test_accuracy_comparisons = dict()\n",
    "model_valid_accuracy_comparisons = dict()\n",
    "model_tuned_params_list = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stack layer 1st which is Decision tree\n",
    "pred1 = clf1.predict(X_test1)\n",
    "\n",
    "#Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test1, pred1) # , normalize=True, sample_weight=None\n",
    "model_valid_accuracy_comparisons[\"DT_Stack_layer\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test1, pred1))\n",
    "\n",
    "#Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test1, pred1))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test1), pred1, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2nd stack layer which is Logistice Regression\n",
    "pred2 = clf2.predict(X_test1)\n",
    "\n",
    "#Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test1, pred2) # , normalize=True, sample_weight=None\n",
    "model_valid_accuracy_comparisons[\"LR_Stack_layer\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test1, pred2))\n",
    "\n",
    "#Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test1, pred2))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test1), pred2, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the cross validation performed on the Super Learner classifiers.\n",
    "my_model = SuperLearnerClassifier()\n",
    "scores = cross_val_score(my_model, X, y, cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the comparision between both the stack layer classifier with hyperparameters\n",
      "DT Stack Accuracy:                    Accuracy\n",
      "Prediction   0.858333333333\n",
      "Probability  0.141666666667\n",
      "Logistice_Reg Stack Accuracy:                    Accuracy\n",
      "Prediction   0.608333333333\n",
      "Probability             0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the comparision between both the stack layer classifier with hyperparameters\")\n",
    "\n",
    "dtclf1 = SuperLearnerClassifier(k=4, stackclf=tree.DecisionTreeClassifier(), pre_pro=False, Ori_Des=False)\n",
    "dtclf1.fit(X,y)\n",
    "dtpre1=dtclf1.predict(X_test1)\n",
    "accuracydt1 = metrics.accuracy_score(y_test1, dtpre1)\n",
    "\n",
    "dtclfp2 = SuperLearnerClassifier(k=4, stackclf=tree.DecisionTreeClassifier(), pre_pro=True, Ori_Des=False)\n",
    "dtclfp2.fit(X,y)\n",
    "dtprep2=dtclfp2.predict(X_test1)\n",
    "accuracydtp2 = metrics.accuracy_score(y_test1, dtprep2)\n",
    "\n",
    "data = np.array([['','Accuracy'],['Prediction',accuracydt1], ['Probability',accuracydtp2]])\n",
    "                \n",
    "AccuracyDT=pd.DataFrame(data=data[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"DT Stack Accuracy:\",AccuracyDT)\n",
    "\n",
    "\n",
    "lrclf1 = SuperLearnerClassifier(k=4, stackclf=linear_model.LogisticRegression(), pre_pro=False, Ori_Des=False)\n",
    "lrclf1.fit(X,y)\n",
    "lrpre1=lrclf1.predict(X_test1)\n",
    "accuracylr1 = metrics.accuracy_score(y_test1, lrpre1)\n",
    "\n",
    "lrclf2 = SuperLearnerClassifier(k=4, stackclf=linear_model.LogisticRegression(), pre_pro=True, Ori_Des=False)\n",
    "lrclf2.fit(X,y)\n",
    "lrpre2=lrclf2.predict(X_test1)\n",
    "accuracylr2 = metrics.accuracy_score(y_test1, lrpre2)\n",
    "\n",
    "\n",
    "datalr = np.array([['','Accuracy'],['Prediction',accuracylr1], ['Probability',accuracylr2]])\n",
    "                \n",
    "Accuracylr=pd.DataFrame(data=datalr[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"Logistice_Reg Stack Accuracy:\", Accuracylr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I perform grid serach on the both type of stack classifier below\n",
    "\n",
    "l = list()\n",
    "\n",
    "param_grid ={'criterion': ['gini', \"entropy\"], \\\n",
    "             'max_depth': list(range(3, 50, 3)), \\\n",
    "             'min_samples_split': [200]}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree = GridSearchCV(tree.DecisionTreeClassifier(), \\\n",
    "                                param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                            return_train_score=True)\n",
    "my_tuned_tree.fit(X_train1, y_train1)\n",
    "\n",
    "# Print details\n",
    "#print(\"Best parameters set found on Descision Tree:\")\n",
    "#display(my_tuned_tree.best_params_)\n",
    "k.append(my_tuned_tree.best_params_)\n",
    "\n",
    "param_grid1 = [\n",
    " {'multi_class': ['ovr'], \n",
    " 'C': [x / 10.0 for x in range(2, 21, 2)],\n",
    " 'solver':['liblinear'],\n",
    "  'max_iter':[1000]}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model1 = GridSearchCV(linear_model.LogisticRegression(), param_grid1, cv=cv_folds, verbose = 2)\n",
    "my_tuned_model1.fit(X_train1, y_train1)\n",
    "\n",
    "# Print details\n",
    "#print(\"Best parameters set found on development set:\")\n",
    "#print(my_tuned_model1.best_params_)\n",
    "l.append(my_tuned_model1.best_params_)\n",
    "\n",
    "param_grid2 = [\n",
    "               {'n_neighbors': list(range(1, 50, 5))}\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Best Parameters list\")\n",
    "for i in k:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the grid search code for getting the good parameters for the Super learner classifiers.\n",
    "param_grid ={'stackclf': [linear_model.LogisticRegression(), tree.DecisionTreeClassifier()], \\\n",
    "             'pre_pro': ['True', 'False']}\n",
    "\n",
    "# Performing the search below:\n",
    "my_tuned_tree = GridSearchCV(SuperLearnerClassifier(), \\\n",
    "                                param_grid, cv=5, verbose = 2, \\\n",
    "                            return_train_score=True)\n",
    "my_tuned_tree.fit(X, y)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree.best_params_)\n",
    "model_tuned_params_list[\"Tuned Tree\"] = my_tuned_tree.best_params_\n",
    "display(my_tuned_tree.best_score_)\n",
    "display(my_tuned_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluting the impact of adding the orignal feature I have used the parameter called Ori_Des. Below is the comparision of the accuracy before and afte adding the Orignal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the comparision between classifier before and after adding the orignal features\n",
      "this is for checking\n",
      "this is for checking\n",
      "this is for checking\n",
      "this is for checking\n",
      "DT_Stack Accuracy With Orignal Features:                  Accuracy\n",
      "Preb&Orig  0.983333333333\n",
      "Prob&Orig           0.125\n",
      "DT Stack Accuracy without Orignal Feature:                    Accuracy\n",
      "Preb&NoOrig  0.883333333333\n",
      "Pr0b&NoOrig  0.433333333333\n",
      "this is for checking\n",
      "this is for checking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateek\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is for checking\n",
      "this is for checking\n",
      "LR_Stack Accuracy With Orignal Features:                  Accuracy\n",
      "Preb&Orig  0.933333333333\n",
      "Prob&Orig  0.216666666667\n",
      "LR_Stack Accuracy without Orignal Features:                    Accuracy\n",
      "Preb&NoOrig  0.441666666667\n",
      "Pr0b&NoOrig  0.858333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the comparision between classifier before and after adding the orignal features\")\n",
    "\n",
    "# In this code orignal feature is not added as flag named Ori_Des is set False\n",
    "dtclf1 = SuperLearnerClassifier(k=3, stackclf=tree.DecisionTreeClassifier(), pre_pro=False, Ori_Des=False)\n",
    "dtclf1.fit(X,y)\n",
    "dtpre1=dtclf1.predict(X_test1)\n",
    "accuracydt1 = metrics.accuracy_score(y_test1, dtpre1)\n",
    "\n",
    "# In this code orignal feature is added as flag named Ori_Des is set True\n",
    "dtclf2 = SuperLearnerClassifier(k=3, stackclf=tree.DecisionTreeClassifier(), pre_pro=False, Ori_Des=True)\n",
    "dtclf2.fit(X,y)\n",
    "dtpre2=dtclf2.predict(X_test1)\n",
    "accuracydt2 = metrics.accuracy_score(y_test1, dtpre2)\n",
    "\n",
    "# In this code orignal feature is added as flag named Ori_Des is set True\n",
    "dtclf3 = SuperLearnerClassifier(k=3, stackclf=tree.DecisionTreeClassifier(), pre_pro=True, Ori_Des=True)\n",
    "dtclf3.fit(X,y)\n",
    "dtpre3=dtclf3.predict(X_test1)\n",
    "accuracydt3 = metrics.accuracy_score(y_test1, dtpre3)\n",
    "\n",
    "# In this code orignal feature is not added as flag named Ori_Des is set False\n",
    "dtclf4 = SuperLearnerClassifier(k=3, stackclf=tree.DecisionTreeClassifier(), pre_pro=True, Ori_Des=False)\n",
    "dtclf4.fit(X,y)\n",
    "dtpre4=dtclf4.predict(X_test1)\n",
    "accuracydt4 = metrics.accuracy_score(y_test1, dtpre4)\n",
    "\n",
    "data = np.array([['','Accuracy'],['Preb&Orig',accuracydt2], ['Prob&Orig',accuracydt3]])\n",
    "                \n",
    "AccuracyDTWithOrig=pd.DataFrame(data=data[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"DT_Stack Accuracy With Orignal Features:\",AccuracyDTWithOrig)\n",
    "\n",
    "data = np.array([['','Accuracy'],['Preb&NoOrig',accuracydt1], ['Pr0b&NoOrig',accuracydt4]])\n",
    "                \n",
    "AccuracyDTWithOutOri=pd.DataFrame(data=data[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"DT Stack Accuracy without Orignal Feature:\",AccuracyDTWithOutOri)\n",
    "\n",
    "\n",
    "lrclf1 = SuperLearnerClassifier(k=3, stackclf=linear_model.LogisticRegression(), pre_pro=False, Ori_Des=False)\n",
    "lrclf1.fit(X,y)\n",
    "lrpre1=lrclf1.predict(X_test1)\n",
    "accuracylr1 = metrics.accuracy_score(y_test1, lrpre1)\n",
    "\n",
    "lrclf2 = SuperLearnerClassifier(k=3, stackclf=linear_model.LogisticRegression(), pre_pro=False, Ori_Des=True)\n",
    "lrclf2.fit(X,y)\n",
    "lrpre2=lrclf2.predict(X_test1)\n",
    "accuracylr2 = metrics.accuracy_score(y_test1, lrpre2)\n",
    "\n",
    "lrclf3 = SuperLearnerClassifier(k=3, stackclf=linear_model.LogisticRegression(), pre_pro=True, Ori_Des=True)\n",
    "lrclf3.fit(X,y)\n",
    "lrpre3=lrclf3.predict(X_test1)\n",
    "accuracylr3 = metrics.accuracy_score(y_test1, lrpre3)\n",
    "\n",
    "lrclf4 = SuperLearnerClassifier(k=3, stackclf=linear_model.LogisticRegression(), pre_pro=True, Ori_Des=False)\n",
    "lrclf4.fit(X,y)\n",
    "lrpre4=lrclf4.predict(X_test1)\n",
    "accuracylr4 = metrics.accuracy_score(y_test1, lrpre4)\n",
    "\n",
    "data = np.array([['','Accuracy'],['Preb&Orig',accuracylr2], ['Prob&Orig',accuracylr3]])\n",
    "                \n",
    "AccuracyLRWithOrig=pd.DataFrame(data=data[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"LR_Stack Accuracy With Orignal Features:\",AccuracyLRWithOrig)\n",
    "\n",
    "data = np.array([['','Accuracy'],['Preb&NoOrig',accuracylr1], ['Pr0b&NoOrig',accuracylr4]])\n",
    "                \n",
    "AccuracyLRWithOutOri=pd.DataFrame(data=data[1:,1:],index=data[1:,0], columns=data[0,1:])\n",
    "\n",
    "print(\"LR_Stack Accuracy without Orignal Features:\",AccuracyLRWithOutOri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       1.00      1.00      1.00        11\n",
      "          5       0.90      1.00      0.95         9\n",
      "          6       0.85      0.92      0.88        12\n",
      "          7       1.00      0.93      0.97        15\n",
      "          8       1.00      0.94      0.97        18\n",
      "          9       0.89      0.89      0.89         9\n",
      "\n",
      "avg / total       0.96      0.96      0.96       120\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  3   4   5   6   7   8  9  All\n",
       "True                                                \n",
       "0          11   0   0  0   0   0   0   0   0  0   11\n",
       "1           0  11   0  0   0   0   0   0   0  0   11\n",
       "2           0   0  14  0   0   0   0   0   0  0   14\n",
       "3           0   0   0  9   0   0   1   0   0  0   10\n",
       "4           0   0   0  0  11   0   0   0   0  0   11\n",
       "5           0   0   0  0   0   9   0   0   0  0    9\n",
       "6           1   0   0  0   0   0  11   0   0  0   12\n",
       "7           0   0   0  0   0   0   0  14   0  1   15\n",
       "8           0   0   0  0   0   0   1   0  17  0   18\n",
       "9           0   0   0  0   0   1   0   0   0  8    9\n",
       "All        12  11  14  9  11  10  13  14  17  9  120"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pre = clf1.predict(X_test1)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test1, dt_pre) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"DTree_Stack\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test1, dt_pre))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test1), dt_pre, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       1.00      1.00      1.00        10\n",
      "          4       0.92      1.00      0.96        11\n",
      "          5       1.00      1.00      1.00         9\n",
      "          6       1.00      0.75      0.86        12\n",
      "          7       1.00      1.00      1.00        15\n",
      "          8       0.95      1.00      0.97        18\n",
      "          9       0.90      1.00      0.95         9\n",
      "\n",
      "avg / total       0.98      0.97      0.97       120\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2   3   4  5  6   7   8   9  All\n",
       "True                                                \n",
       "0          11   0   0   0   0  0  0   0   0   0   11\n",
       "1           0  11   0   0   0  0  0   0   0   0   11\n",
       "2           0   0  14   0   0  0  0   0   0   0   14\n",
       "3           0   0   0  10   0  0  0   0   0   0   10\n",
       "4           0   0   0   0  11  0  0   0   0   0   11\n",
       "5           0   0   0   0   0  9  0   0   0   0    9\n",
       "6           0   0   0   0   1  0  9   0   1   1   12\n",
       "7           0   0   0   0   0  0  0  15   0   0   15\n",
       "8           0   0   0   0   0  0  0   0  18   0   18\n",
       "9           0   0   0   0   0  0  0   0   0   9    9\n",
       "All        11  11  14  10  12  9  9  15  19  10  120"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pre = clf2.predict(X_test1)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test1, lr_pre) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"LR_Stack\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test1, lr_pre))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test1), lr_pre, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DTree_Stack': 0.95833333333333337, 'LR_Stack': 0.97499999999999998}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_test_accuracy_comparisons)   # to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DT_Stack_layer': 0.84166666666666667, 'LR_Stack_layer': 0.9916666666666667}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_valid_accuracy_comparisons)   # to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAD8CAYAAAAmL+CoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e8a428b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')\n",
    "_ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95034722])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find correation between both the stack layer classifier:\n",
    "b = [accuracydtS]\n",
    "t = [accuracylrS]\n",
    "np.correlate(b,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
